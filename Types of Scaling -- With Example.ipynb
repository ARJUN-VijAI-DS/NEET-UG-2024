{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44cb9835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaled Data:\n",
      "   Feature1  Feature2  Feature3\n",
      "0  0.000000  0.000000       0.2\n",
      "1  0.487179  0.210526       0.0\n",
      "2  0.051282  0.473684       0.6\n",
      "3  1.000000  0.736842       0.4\n",
      "4  0.102564  1.000000       1.0\n",
      "\n",
      "Standard Scaled Data:\n",
      "   Feature1  Feature2  Feature3\n",
      "0 -0.869803 -1.354113 -0.697486\n",
      "1  0.421311 -0.765368 -1.278724\n",
      "2 -0.733896 -0.029437  0.464991\n",
      "3  1.780378  0.706494 -0.116248\n",
      "4 -0.597989  1.442425  1.627467\n",
      "\n",
      "Robust Scaled Data:\n",
      "   Feature1  Feature2  Feature3\n",
      "0 -0.235294      -0.9      -0.5\n",
      "1  0.882353      -0.5      -1.0\n",
      "2 -0.117647       0.0       0.5\n",
      "3  2.058824       0.5       0.0\n",
      "4  0.000000       1.0       1.5\n",
      "\n",
      "MaxAbs Scaled Data:\n",
      "   Feature1  Feature2  Feature3\n",
      "0     0.025      0.05  0.333333\n",
      "1     0.500      0.25  0.166667\n",
      "2     0.075      0.50  0.666667\n",
      "3     1.000      0.75  0.500000\n",
      "4     0.125      1.00  1.000000\n",
      "\n",
      "Quantile Scaled Data:\n",
      "   Feature1  Feature2  Feature3\n",
      "0 -5.199338 -5.199338 -0.674490\n",
      "1  0.674490 -0.674490 -5.199338\n",
      "2 -0.674490  0.000000  0.674490\n",
      "3  5.199338  0.674490  0.000000\n",
      "4  0.000000  5.199338  5.199338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2627: UserWarning: n_quantiles (1000) is greater than the total number of samples (5). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler, MaxAbsScaler, QuantileTransformer\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'Feature1': [10, 200, 30, 400, 50],\n",
    "    'Feature2': [1, 5, 10, 15, 20],\n",
    "    'Feature3': [1000, 500, 2000, 1500, 3000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Min-Max Scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_min_max_scaled = min_max_scaler.fit_transform(df)\n",
    "df_min_max_scaled = pd.DataFrame(df_min_max_scaled, columns=df.columns)\n",
    "\n",
    "# Standardization\n",
    "standard_scaler = StandardScaler()\n",
    "df_standard_scaled = standard_scaler.fit_transform(df)\n",
    "df_standard_scaled = pd.DataFrame(df_standard_scaled, columns=df.columns)\n",
    "\n",
    "# Robust Scaling\n",
    "robust_scaler = RobustScaler()\n",
    "df_robust_scaled = robust_scaler.fit_transform(df)\n",
    "df_robust_scaled = pd.DataFrame(df_robust_scaled, columns=df.columns)\n",
    "\n",
    "# MaxAbs Scaling\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "df_max_abs_scaled = max_abs_scaler.fit_transform(df)\n",
    "df_max_abs_scaled = pd.DataFrame(df_max_abs_scaled, columns=df.columns)\n",
    "\n",
    "# Quantile Transformer Scaling (normal distribution)\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "df_quantile_scaled = quantile_transformer.fit_transform(df)\n",
    "df_quantile_scaled = pd.DataFrame(df_quantile_scaled, columns=df.columns)\n",
    "\n",
    "# Print scaled dataframes\n",
    "print(\"Min-Max Scaled Data:\")\n",
    "print(df_min_max_scaled)\n",
    "print(\"\\nStandard Scaled Data:\")\n",
    "print(df_standard_scaled)\n",
    "print(\"\\nRobust Scaled Data:\")\n",
    "print(df_robust_scaled)\n",
    "print(\"\\nMaxAbs Scaled Data:\")\n",
    "print(df_max_abs_scaled)\n",
    "print(\"\\nQuantile Scaled Data:\")\n",
    "print(df_quantile_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1692b87",
   "metadata": {},
   "source": [
    "## Choosing the Right Scaling Method\n",
    "\n",
    "- Min-Max Scaling: When the features are not normally distributed and you want to retain the relationships in the data.<br><br>\n",
    "\n",
    "- Standardization: When the data is normally distributed or when using algorithms that assume normally distributed data.<br><br>\n",
    "\n",
    "- Robust Scaling: When the data contains outliers.<br><br>\n",
    "\n",
    "- MaxAbs Scaling: When dealing with sparse data.<br><br>\n",
    "\n",
    "- Quantile Transformer Scaling: When you want to transform the data to follow a specific distribution.<br><br>\n",
    "\n",
    "**Choose the scaling method that best fits the characteristics of your dataset and the requirements of your machine learning algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37bf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "420def6e",
   "metadata": {},
   "source": [
    "## What is Sparse Data?\n",
    "\n",
    "Sparse data refers to datasets where the majority of the values are zeros or are absent. This kind of data structure is common in various fields such as natural language processing, recommendation systems, and more. Here are some key characteristics and examples of sparse data:\n",
    "\n",
    "Characteristics of Sparse Data:\n",
    "High Dimensionality: Sparse data often has a large number of dimensions (features or variables) but only a small fraction of the possible values are non-zero.\n",
    "Many Zero Entries: The matrix or dataset contains a high proportion of zero values compared to non-zero values.\n",
    "Storage Efficiency: Sparse data can be stored more efficiently using specialized data structures that only store the non-zero entries.\n",
    "Examples of Sparse Data:\n",
    "Text Data in NLP:\n",
    "\n",
    "When representing text data using a bag-of-words or TF-IDF model, each document is represented as a vector of word counts or frequencies. Since any given document uses only a small subset of all possible words, the resulting matrix is sparse.\n",
    "Recommendation Systems:\n",
    "\n",
    "User-item ratings matrices in recommendation systems are typically sparse because any given user has rated only a small fraction of all available items.\n",
    "One-Hot Encoding:\n",
    "\n",
    "When categorical variables are converted into binary vectors through one-hot encoding, the resulting vectors are sparse since only one element is '1' and the rest are '0'.\n",
    "Image Data:\n",
    "\n",
    "In some applications, such as certain types of image processing or computer vision tasks, the data can be sparse if most of the pixels in an image are background (zeros) and only a few pixels contain meaningful information (non-zeros).\n",
    "Handling Sparse Data:\n",
    "Handling sparse data effectively is crucial for efficient storage and computation. Some common techniques and data structures used for sparse data include:\n",
    "\n",
    "Compressed Sparse Row (CSR) and Compressed Sparse Column (CSC):\n",
    "\n",
    "These are efficient storage formats for sparse matrices where only the non-zero entries and their row/column indices are stored.\n",
    "Scipy's Sparse Module:\n",
    "\n",
    "In Python, the scipy.sparse module provides various functions and classes to create and manipulate sparse matrices.\n",
    "Sparse Data Structures:\n",
    "\n",
    "Libraries like Pandas and NumPy also offer support for sparse data structures to efficiently handle sparse datasets.\n",
    "Example in Python:\n",
    "Here's an example of creating and manipulating sparse matrices using the scipy.sparse module in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1e9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e883713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix (CSR):\n",
      "  (0, 2)\t1\n",
      "  (1, 0)\t2\n",
      "  (2, 3)\t3\n",
      "  (3, 1)\t4\n",
      "\n",
      "Data in Sparse Matrix:\n",
      "[1 2 3 4]\n",
      "\n",
      "Converted Back to Dense Matrix:\n",
      "[[0 0 1 0]\n",
      " [2 0 0 0]\n",
      " [0 0 0 3]\n",
      " [0 4 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Example dense matrix\n",
    "dense_matrix = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [2, 0, 0, 0],\n",
    "    [0, 0, 0, 3],\n",
    "    [0, 4, 0, 0]\n",
    "])\n",
    "\n",
    "# Convert dense matrix to CSR (Compressed Sparse Row) format\n",
    "sparse_matrix = csr_matrix(dense_matrix)\n",
    "\n",
    "# Print sparse matrix\n",
    "print(\"Sparse Matrix (CSR):\")\n",
    "print(sparse_matrix)\n",
    "\n",
    "# Accessing data in sparse matrix\n",
    "print(\"\\nData in Sparse Matrix:\")\n",
    "print(sparse_matrix.data)\n",
    "\n",
    "# Converting sparse matrix back to dense format\n",
    "dense_matrix_back = sparse_matrix.toarray()\n",
    "print(\"\\nConverted Back to Dense Matrix:\")\n",
    "print(dense_matrix_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc3a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
